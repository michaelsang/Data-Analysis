1. Cleaning and Exploring Big Data using PySpark
Cleaning and exploring big data in PySpark is quite different from Python due to the distributed nature of Spark dataframes. 
This project will dive deep into various ways to clean and explore data loaded in PySpark. 
Data preprocessing in big data analysis is a crucial step before building any big data machine learning model. 

Cleaning
Python Programming
Data Visualization (DataViz)
Apache Spark
Exploratory Data Analysis

Install Spark on Google Colab and load datasets in PySpark
Change column datatype, remove whitespaces and drop duplicates
Remove columns with Null values higher than a threshold
Group, aggregate and create pivot tables
Rename categories and impute missing numeric values
Create visualizations to gather insights



2. Data Analysis Using Pyspark.
Distributed data processing technologies. As a data analyst, apply different queries to your dataset to extract useful information out of it. The distributed data processing and 
Spark Technology will become handy if your data is 
so big that working with it on your local machine is not easy to be done. In this project, we work with pyspark module in python and we are going to use google colab environment in order to apply some 
queries to the dataset we have related to lastfm website which is an online music service where users can listen to different songs. 
This dataset is containing two csv files listening.csv and genre.csv. Also, we will learn how we can visualize our query results using matplotlib.

Google colab
Data Analysis
Python Programming
pySpark SQL

Prepare the Google Colab for distributed data processing
Mounting our Google Drive into Google Colab environment
Importing first file of our Dataset (1 Gb) into pySpark dataframe
Applying some Queries to extract useful information out of our data
Importing second file of our Dataset (3 Mb) into pySpark dataframe
Joining two dataframes and prepapre it for more advanced queries
Learn visualizing our query results using matplotlib



3. Building Machine Learning Pipelines in PySpark MLlib.
This project creates machine learning pipelines using Python and Spark, free, open-source programs that you can download. You will learn how to load your dataset in Spark and learn how to perform basic cleaning techniques such as removing columns with high missing values and removing rows with missing values. You will then create a machine learning pipeline with a random forest regression model. You will use cross validation and parameter tuning to select the best model from the pipeline. Lastly, you will evaluate your model’s performance using various metrics.

A pipeline in Spark combines multiple execution steps in the order of their execution. So rather than executing the steps individually, one can put them in a pipeline to streamline the machine learning process. You can save this pipeline, share it with your colleagues, and load it back again effortlessly.


Install Spark on Google Colab and load a dataset in PySpark
Describe and clean your dataset
Create a Random Forest pipeline to predict car prices
Create a cross validator for hyperparameter tuning
Train your model and predict test set car prices
Evaluate your model’s performance via several metrics


4. Graduate Admission Prediction with Pyspark ML
you will learn to build a linear regression model using Pyspark ML to predict students' admission at the university. We will use the graduate admission data set from Kaggle. Our goal is to use a Simple Linear Regression Machine Learning Algorithm from the Pyspark Machine learning library to predict the chances of getting admission. We will be carrying out the entire project on the Google Colab environment with the installation of Pyspark. The dataset and the model in this project can not be used in the real-life. 

By the end of this project, you will be able to build the linear regression model using Pyspark ML to predict admission chances. You will also be able to setup and work with Pyspark on the Google Colab environment. Additionally, you will also be able to clean and prepare data for analysis.

Introduction and Installing Dependencies
Clone and Explore the Dataset
Data Cleaning
Correlation analysis and Feature Selection
Build the Linear Regression Model
Evaluate and Test the model



5. Diabetes Prediction With Pyspark MLLIB
You will learn to build a logistic regression model using Pyspark MLLIB to classify patients as either diabetic or non-diabetic. We will use the popular Pima Indian Diabetes data set. Our goal is to use a simple logistic regression classifier from the pyspark Machine learning library for diabetes classification. We will be carrying out the entire project on the Google Colab environment with the installation of Pyspark.You will need a free Gmail account to complete this project. Please be aware of the fact that the dataset and the model in this project, can not be used in the real-life. We are only using this data for the educational purpose.

By the end of this project, you will be able to build the logistic regression classifier using Pyspark MLlib to classify between the diabetic and nondiabetic patients.You will also be able to setup and work with  Pyspark on Google colab environment. Additionally, you will also be able to clean and prepare data for analysis.


Introduction & Install Dependencies
Clone and Explore Dataset 
Data Cleaning and Preparation
Correlation analysis and Feature Selection
Split Dataset and Build the Logistic Regression Model
Evaluate and Save the model
Model Prediction on a new set of unlabelled data


