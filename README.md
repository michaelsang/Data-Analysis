# Pyspark

1. Cleaning and exploring big data in PySpark is quite different from Python due to the distributed nature of Spark dataframes. 
This project will dive deep into various ways to clean and explore data loaded in PySpark. 
Data preprocessing in big data analysis is a crucial step before building any big data machine learning model. 

Cleaning
Python Programming
Data Visualization (DataViz)
Apache Spark
Exploratory Data Analysis

Install Spark on Google Colab and load datasets in PySpark
Change column datatype, remove whitespaces and drop duplicates
Remove columns with Null values higher than a threshold
Group, aggregate and create pivot tables
Rename categories and impute missing numeric values
Create visualizations to gather insights



2. Distributed data processing technologies. As a data analyst, apply different queries to your dataset to extract useful information out of it. But what if your data is 
so big that working with it on your local machine is not easy to be done. That is when the distributed data processing and 
Spark Technology will become handy. So in this project, we work with pyspark module in python and we are going to use google colab environment in order to apply some 
queries to the dataset we have related to lastfm website which is an online music service where users can listen to different songs. 
This dataset is containing two csv files listening.csv and genre.csv. Also, we will learn how we can visualize our query results using matplotlib.

Google colab
Data Analysis
Python Programming
pySpark SQL

Prepare the Google Colab for distributed data processing
Mounting our Google Drive into Google Colab environment
Importing first file of our Dataset (1 Gb) into pySpark dataframe
Applying some Queries to extract useful information out of our data
Importing second file of our Dataset (3 Mb) into pySpark dataframe
Joining two dataframes and prepapre it for more advanced queries
Learn visualizing our query results using matplotlib


